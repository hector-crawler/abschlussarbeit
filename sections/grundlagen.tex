\section{Grundlagen}

% Einführung in voraugesetzte theoretische Grundlagen; hier viele Zitate

\subsection{Robot Operating System (ROS)}

Das Robot Operating System (ROS)\cite{ros} ist ein quelloffenes Softwareframework zur Entwicklung von Robotern. Es bildet eine Middleware zur Verwaltung einzelner Softwarekomponenten, sowie deren Kommunikation untereinander. Diese Komponenten (genannt ``Nodes'') können jeweils in einer beliebigen Programmiersprache (hier Python) implementiert, und bei Programmstart von ROS ausgeführt werden. Die Nodes laufen in getrennten Prozessen. Kommunikation zwischen diesen Nodes geschieht über Kanäle in ROS (genannt ``Topics''), auf denen Nachrichten von sog. Publisher-Nodes gesendet und von sog. Subscriber-Nodes gelesen werden können. Die Datentypen dieser Nachrichten können pro Topic festgelegt werden. Diese Datentypen können grundlegende Typen wie Zahlen oder Strings, aber auch komplexere, im Code festgelegte Datenstukturen sein.

\subsection{Der Lernprozess: Reinforcement Learning}
\label{sec:reinforcement_learning}

Im Maschinellen Lernen wird, um selbstlernende Algorithmen zu trainieren, meist das Konzept des Reinforcement Learning (RL) angewendet. Das Konzept besteht in der Regel aus zwei Gegenspielern:
\begin{itemize}
    \item Dem Algorithmus, der (anfangs zufällig, später gezielt) Aktionen durchführt.
    \item Einer Umgebung (dem Environment), welche die Aktionen des Algorithmus anhand einer gegebenen Statistik bewertet, und dem Algorithmus anhanddessen Feedback gibt, den sog. Reward. Diesen Reward benutzt der Algorithmus dann, um sich eigenständig zu verbessern.
\end{itemize}

In der jetzigen Implementation des Crawlers basiert der Reward alleine auf der Veränderung der Radposition. Wenn die Positionen der Inkrementalgeber auf eine Vorwärtsbewegung schließen lassen, erhält der Algorithmus einen positiven Reward. Bei einer Rückwärtsbewegung oder keiner Veränderung ist der Reward negativ.

\subsubsection{Der Lernprozess: Das Q-Learning}

Das Q-Learning ist ein Algorithmus, der auf die Auswahl einer Aktion aus einer Reihe an Möglichkeiten spezialisiert ist. Dem Algorithmus liegt die Q-Table zugrunde, eine Tabelle, in der jede Zelle eine spezifische Kombination aus einer Input-Option (einem Zustand der Umgebung) und einer Output-Option (einer Aktion) darstellt. Jede Zelle ist außerdem mit einer Zahl besetzt. Soll der Algorithmus eine Aktion auswählen, wählt er aus den Zellen, deren Input-Option dem Zustand der Umgebung entspricht, die Zelle, deren Zahl am höchsten ist. Die zugehörige Aktion ist dann das Ergebnis des Algorithmus. Wird das Q-Learning in Kombination mit dem Konzept des RL angewandt, wird die Zahl in der ausgewählten Zelle je nach resultierendem Reward angepasst: Bei positivem Reward wird sie vergrößert, bei negativem Reward verringert. Um den Einfluss verschiedener Aktionen auf die Umgebung zu erkunden, wird der Output anfangs noch ab und zu zufällig ausgewählt, mit voranschreitender Wiederholungszahl passiert dies allerdings immer seltener.

In der Q-Learning-Implementation des Crawlers werden die Positionen der Motoren an Arm- und Hand-Gelenk des Roboters als Input-Optionen genutzt. Um die Dimensionen der Q-Table zu begrenzen, werden die Positionen allerdings nur grob als ``oben'', ``mittig'' oder ``unten'' ausgelesen. Output-Optionen sind ``Arm hoch'', ``Arm runter'', ``Hand hoch'' und ``Hand runter''.

\begin{table}
	\captionsetup{justification=centering}
	\caption{Beispielhafte Visualisierung der Q-Table beim Crawler \\ (Zahlen entsprechen keinem realen Szenario)}
	\centering
	\begin{tabular}{ |c|c|cccc| }
		\hline
	\multirow{2}{4em}{Zustand Arm} & \multirow{2}{4em}{Zustand Hand} & \multicolumn{4}{|c|}{Aktionen} \\
		& & Arm hoch & Arm runter             & Hand hoch & Hand runter \\
		\hline
	\multirow{3}{4em}{Oben} & Oben & 0.52 & 0.64 & 0.83 & 0.21 \\
		& Mittig & 0.80 & 0.06 & 0.75 & 0.66 \\
		& Unten & 0.46 & 0.85 & 0.51 & 0.67 \\
		\hline
	\multirow{3}{4em}{Mittig} & Oben & 0.13 & 0.88 & 0.81 & 0.10 \\
		& Mittig & 0.76 & 0.42 & 0.90 & 0.66 \\
		& Unten & 0.28 & 0.73 & 0.52 & 0.25 \\
	    \hline
	\multirow{3}{4em}{Unten} & Oben & 0.75 & 0.90 & 0.34 & 0.49 \\
		& Mittig & 0.93 & 0.22 & 0.26 & 0.62 \\
		& Unten & 0.89 & 0.62 & 0.59 & 0.57 \\
	    \hline
	\end{tabular}
% \label{fig:example-q-table}
\end{table}
